{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Deep Learning Models for Predicting Export Value of Crops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Libraries \n",
        "import os\n",
        "from azure.identity import (DefaultAzureCredential, InteractiveBrowserCredential)\n",
        "from azure.ai.ml import (MLClient, Input, command)\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "from azure.ai.ml.sweep import (Choice, BanditPolicy, Uniform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x7fdc7a3af6a0>,\n",
            "         subscription_id=567f42ad-44d7-4850-aada-1d69d5b9aae9,\n",
            "         resource_group_name=dp-100rg,\n",
            "         workspace_name=projectomegadev)\n"
          ]
        }
      ],
      "source": [
        "# Connect to work space \n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    credential = InteractiveBrowserCredential()\n",
        "    \n",
        "# get token \n",
        "ws = MLClient.from_config(credential=credential)\n",
        "print(ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/azureuser/cloudfiles/code/Users/deborahashante/MLProject/src folder created\n"
          ]
        }
      ],
      "source": [
        "# Create cloud folder\n",
        "local_dir = \"/home/azureuser/cloudfiles/code/Users/deborahashante/MLProject/src\"\n",
        "if not os.path.exists(local_dir):\n",
        "    os.mkdir(local_dir)\n",
        "    \n",
        "print(local_dir, 'folder created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create Python script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /home/azureuser/cloudfiles/code/Users/deborahashante/MLProject/src/train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $local_dir/train.py \n",
        "# import libraries \n",
        "import argparse\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import mlflow\n",
        "\n",
        "# define the main function\n",
        "def main(args):\n",
        "    \"\"\"\n",
        "    The main function invokes other sub routines in the pipeline\n",
        "    \"\"\"\n",
        "    # enable autolog \n",
        "    mlflow.tensorflow.autolog()\n",
        "    \n",
        "    # get the data \n",
        "    df = get_mlp_data(args.training_data)\n",
        "    \n",
        "    # execute the preprocess data function \n",
        "    X, Y = preprocess_data(df)\n",
        "    \n",
        "    # split the dataset \n",
        "    X_train,X_test,Y_train,Y_test = split_data(X, Y)\n",
        "    \n",
        "    # train MLP model \n",
        "    model, history = train_model(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        Y_train,\n",
        "        Y_test,\n",
        "        args.regs,\n",
        "        args.num_neurons,\n",
        "        args.num_layers,\n",
        "        args.epoch,\n",
        "        args.learning_rate,\n",
        "        args.batch_size\n",
        "    )\n",
        "    \n",
        "    # evaluate the model \n",
        "    evaluate_model(model,X_test,Y_test)\n",
        "    \n",
        "    # plot model training graphs\n",
        "    plot_training(history)\n",
        "\n",
        "# define a function to read data \n",
        "def get_mlp_data(data_path):\n",
        "    \"\"\"\n",
        "    This function reads the data from MLTable format to pandas dataframe\n",
        "    \"\"\"\n",
        "    # use URI file data path to load data to dataframe\n",
        "    df = pd.read_csv(data_path)\n",
        "    return df \n",
        "\n",
        "# preprocess the data\n",
        "def preprocess_data(df):\n",
        "    \"\"\" \n",
        "    This function applies required preprocessing steps to the data\n",
        "    \"\"\" \n",
        "    # Initialize LabelEncoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    # Fit and transform the 'Area' column\n",
        "    df['Area'] = label_encoder.fit_transform(df['Area']) \n",
        "    \n",
        "    # Prepare feature matrix X and target vector y\n",
        "    X = df.drop(columns=['Export_Value'])\n",
        "    y = df[['Export_Value']]\n",
        "    \n",
        "    # Normalize training and testing dataset\n",
        "    scaler_x = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "    x = scaler_x.fit_transform(X)\n",
        "    Y = scaler_y.fit_transform(y.values.reshape(-1,1))\n",
        "    \n",
        "    return x,Y\n",
        "\n",
        "# define a function that splits the data \n",
        "def split_data(X,Y):\n",
        "    \"\"\" \n",
        "    Funtion accepts preprocessed data as input and split for training and testing dataset\n",
        "    \"\"\"\n",
        "    # split dataset for training and testing\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=40)\n",
        "    \n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# define architecture for export forecasting model\n",
        "def train_model(X_train,X_test,Y_train,Y_test,regs,num_neurons,num_layers,epoch,learning_rate,batch_size):\n",
        "    \"\"\"\n",
        "    This function defines the architecture and training of ANN model. \n",
        "    Hyper params were used to train model on different set of values. \n",
        "    returns: trained model and training history\n",
        "    \"\"\"\n",
        "    # input layer\n",
        "    inputs = keras.Input(shape=(X_train.shape[1]))\n",
        "    # l2 regularization for preventing overfitting\n",
        "    reg = tf.keras.regularizers.l2(l2=regs)\n",
        "    # first hidden layer\n",
        "    x = keras.layers.Dense(num_neurons, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=reg)(inputs)\n",
        "    # efficiently add more layers\n",
        "    for _ in range(num_layers - 1):\n",
        "        x = keras.layers.Dense(num_neurons, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=reg)(x)\n",
        "        # Add dropout layer for regularization\n",
        "        x = keras.layers.Dropout(0.2)(x)\n",
        "    # output layer\n",
        "    outputs = keras.layers.Dense(1)(x)\n",
        "    # set model\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='export_model')\n",
        "\n",
        "    # Compile the model for a regression problem\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        loss='mae',\n",
        "        optimizer=opt,\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    # Early stopping callback\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, Y_train, epochs=epoch, batch_size=batch_size, verbose=1,validation_data=(X_test, Y_test), callbacks=[early_stopping])\n",
        "\n",
        "    # Save performance\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "\n",
        "    # print performance\n",
        "    print('Cost function at epoch 0')\n",
        "    print('Training MAE=', hist['loss'].values[0])\n",
        "    print('Validation MAE=', hist['val_loss'].values[0])\n",
        "    print('Cost function at epoch:', str(epoch))\n",
        "    print('Training MAE=', hist['loss'].values[-1])\n",
        "    print('Validation MAE=', hist['val_loss'].values[-1])\n",
        "    return model, hist\n",
        "\n",
        "# define a function that evaluates model performance\n",
        "def evaluate_model(model,X_test,Y_test):\n",
        "    \"\"\"\n",
        "    Evaluate model on Regression evaluation metrics. \n",
        "    \"\"\"\n",
        "    # predict with model\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "    mean_abs_errors = (mean_absolute_error(Y_test, y_pred))\n",
        "    root_mean_sqrd_error = np.sqrt(mean_squared_error(Y_test, y_pred))\n",
        "    r2 = np.round(r2_score(Y_test, y_pred)*100,2)\n",
        "    \n",
        "    print(\"MAE:\",mean_abs_errors)\n",
        "    print(\"RMSE:\",root_mean_sqrd_error)\n",
        "    print(\"R2:\",r2)\n",
        "\n",
        "# plot training graphs \n",
        "def plot_training(history):\n",
        "    \"\"\" \n",
        "    Plot training history (accuracy and loss)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    # Plot Mean Absolute Error (MAE)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['mae'], label='Training MAE')\n",
        "    plt.plot(history['val_mae'], label='Validation MAE')\n",
        "    plt.title('Model Training History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Mlp.png\")\n",
        "    mlflow.log_artifact(\"Mlp.png\")\n",
        "\n",
        "# parse required arguments for model training \n",
        "def parse_arguments():\n",
        "    \"\"\"\n",
        "    Function that uses python `argparse` method to define hyperparametre values for model training. \n",
        "    \"\"\"\n",
        "    # setup the arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    # add the arguments\n",
        "    parser.add_argument(\"--training_data\", dest='training_data',type=str)\n",
        "    parser.add_argument(\"--regs\", dest='regs',type=float, default=0.001)\n",
        "    parser.add_argument(\"--learning_rate\", dest='learning_rate',type=float, default=0.001)\n",
        "    parser.add_argument(\"--epoch\", dest='epoch',type=int, default=10)\n",
        "    parser.add_argument(\"--batch_size\", dest='batch_size',type=int, default=32)\n",
        "    parser.add_argument(\"--num_neurons\", dest='num_neurons',type=int, default=4)\n",
        "    parser.add_argument(\"--num_layers\", dest='num_layers',type=int, default=4)\n",
        "    \n",
        "    # parse the args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # return args\n",
        "    return args\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # parse args\n",
        "    args = parse_arguments()\n",
        "\n",
        "    # run main function\n",
        "    main(args)\n",
        "\n",
        "    # add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n",
        "    \n",
        "print(\"Created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Configure Command Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32mUploading src (0.01 MBs): 100%|██████████| 7560/7560 [00:00<00:00, 57416.79it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Studio url: \n",
            " https://ml.azure.com/runs/honest_king_bqv2q13g6j?wsid=/subscriptions/567f42ad-44d7-4850-aada-1d69d5b9aae9/resourcegroups/dp-100rg/workspaces/projectomegadev&tid=a684a124-688f-4ad4-a587-1b2c2eb6febf\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# get data\n",
        "data_asset = ws.data.get(\"uri-export-value-data\", version=\"1\")\n",
        "\n",
        "#  create a command job\n",
        "job = command(\n",
        "        code=\"/home/azureuser/cloudfiles/code/Users/deborahashante/MLProject/src/\",\n",
        "        command=\"python train.py --training_data ${{inputs.training_data}} --regs ${{inputs.regs}} --learning_rate ${{inputs.learning_rate}} --epoch ${{inputs.epoch}} --batch_size ${{inputs.batch_size}} --num_neurons ${{inputs.num_neurons}} --num_layers ${{inputs.num_layers}}\",\n",
        "        inputs={\n",
        "            \"training_data\": Input(path=data_asset.id,\n",
        "                type=AssetTypes.URI_FILE,\n",
        "                mode=InputOutputModes.RO_MOUNT\n",
        "            ),\n",
        "            \"regs\": 0.001,\n",
        "            \"learning_rate\": 0.001,\n",
        "            \"epoch\": 10,\n",
        "            \"batch_size\": 32,\n",
        "            \"num_neurons\": 4,\n",
        "            \"num_layers\": 4\n",
        "        },\n",
        "        compute=\"compute-cluster\",\n",
        "        environment=\"export-value-tensorflow-env:5\",\n",
        "        display_name=\"MLP-export-modelv4\",\n",
        "        experiment_name=\"crop-export-modelv4\"\n",
        "    )\n",
        "returned_job = ws.jobs.create_or_update(job)\n",
        "\n",
        "print(f\"Studio url: \\n {returned_job.studio_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create Sweep Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define search space \n",
        "export_value_sweep_job = job(\n",
        "    regs=Choice(values=[0.001,0.01,0.1,1]),\n",
        "    learning_rate=Choice(values=[0.001,0.002,0.01,0.1]),\n",
        "    epoch=Choice(values=[5,10,15,20]),\n",
        "    batch_size=Choice(values=[5,10,15,32]),   \n",
        "    num_neurons=Choice(values=[4,5,6,8]),\n",
        "    num_layers=Choice(values=[4,5,6,8])\n",
        ")\n",
        "\n",
        "# set sampling and early stopping strategy\n",
        "sweep_job = export_value_sweep_job.sweep(\n",
        "    primary_metric=\"r2_score\",\n",
        "    goal=\"Maximize\",\n",
        "    sampling_algorithm = \"grid\",\n",
        "    compute=\"compute-cluster\",\n",
        "    # early_termination_policy=BanditPolicy(\n",
        "    #     slack_amount = 0.1, \n",
        "    #     delay_evaluation = 2, \n",
        "    #     evaluation_interval = 1\n",
        "    # ),\n",
        "    max_total_trials=10,\n",
        "    max_concurrent_trials=5\n",
        ")\n",
        "\n",
        "# set experiment name \n",
        "sweep_job.experiment_name = \"sweepjob-export-value-model\"\n",
        "\n",
        "# submit the sweep\n",
        "returned_sweep_job = ws.create_or_update(sweep_job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/subscriptions/567f42ad-44d7-4850-aada-1d69d5b9aae9/resourceGroups/dp-100rg/providers/Microsoft.MachineLearningServices/workspaces/projectomegadev/jobs/amiable_onion_8hp69mr9bp'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get best perofmed model \n",
        "sweep_job_name = ws.jobs.get(returned_sweep_job.name)\n",
        "sweep_job_name.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Register Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "honest_king_bqv2q13g6j is saved!\n"
          ]
        }
      ],
      "source": [
        "# get job name \n",
        "job_name = returned_job.name \n",
        "\n",
        "# define and save model \n",
        "new_model = Model(\n",
        "    name=\"export-value-model\",\n",
        "    path=f\"azureml://jobs/{job_name}/outputs/artifacts/paths/model/\",\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        "    description=\"register best model\"\n",
        ")\n",
        "\n",
        "# save the model \n",
        "ws.models.create_or_update(new_model)\n",
        "print(f\"{job_name} is saved!\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
